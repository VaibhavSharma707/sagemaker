{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb23c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004e3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, WhisperForConditionalGeneration, WhisperTokenizer, WhisperFeatureExtractor\n",
    "\n",
    "# Load model without forced_decoder_ids\n",
    "model_path = \"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/dev/models/whisper-medical-v2-final\"\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_path)\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None  # Disable forced IDs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c740ff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Make sure to follow the dosage instructions while taking APTOFRESH.                                                                                                                                                                                                                                                                                                                                                                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# 3. Create pipeline with ALL components\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    feature_extractor=feature_extractor,\n",
    "    device=\"cpu\"  # or \"cuda\" if using GPU\n",
    ")\n",
    "\n",
    "# Transcribe audio\n",
    "audio_path = \"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/unitedsynmed_small/audio/test/drug-brand-en-us-female-1beb8df1-473e-4017-9f19-fe8c88afa88d.wav\"\n",
    "\n",
    "result = pipe(audio_path)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937b447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/unitedsynmed_small/audio/train/drug-brand-en-us-female-0d0979f3-3519-4e70-beae-339cda2b254a.wav\"\n",
    "\n",
    "result = pipe(audio_path)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38be4fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Load ONNX model\n",
    "sess = ort.InferenceSession(\"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/dev/models/whisper_medical.onnx\")\n",
    "\n",
    "# Load processor (for tokenization & feature extraction)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "# Manual inference logic needed...\n",
    "def transcribe_audio(audio_path):\n",
    "    # 1. Load and preprocess audio\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)  # Whisper uses 16kHz\n",
    "    \n",
    "    # 2. Extract features\n",
    "    inputs = processor(\n",
    "        audio, \n",
    "        sampling_rate=sr, \n",
    "        return_tensors=\"np\",  # Return NumPy arrays for ONNX\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    # 3. Prepare ONNX inputs\n",
    "    ort_inputs = {\n",
    "        \"input_features\": inputs.input_features.astype(np.float32),\n",
    "        \"decoder_input_ids\": np.array([[processor.tokenizer.bos_token_id]], dtype=np.int64),\n",
    "    }\n",
    "    \n",
    "    # 4. Run inference\n",
    "    ort_outputs = sess.run(None, ort_inputs)\n",
    "    logits = ort_outputs[0]  # First output contains logits\n",
    "    \n",
    "    # 5. Decode predictions\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a812f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "audio_path = \"C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/unitedsynmed_small/audio/train/drug-brand-en-us-female-0d0979f3-3519-4e70-beae-339cda2b254a.wav\"\n",
    "\n",
    "result = transcribe_audio(audio_path)\n",
    "\n",
    "print(\"Transcription:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2acff25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
