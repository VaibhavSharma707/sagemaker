{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech-to-Text with Whisper Transfer Learning\n",
    "\n",
    "**Objective:** Fine-tune a Whisper base model on the United-Syn-Med dataset to improve medical speech transcription accuracy in a live teleconsultation context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T00:47:44.008168Z",
     "iopub.status.busy": "2025-06-02T00:47:44.007869Z",
     "iopub.status.idle": "2025-06-02T00:49:13.542751Z",
     "shell.execute_reply": "2025-06-02T00:49:13.542023Z",
     "shell.execute_reply.started": "2025-06-02T00:47:44.008144Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\ifedi\\appdata\\local\\temp\\pip-req-build-y7g0svdv\n",
      "  Resolved https://github.com/openai/whisper.git to commit dd985ac4b90cafeef8712f2998d62c59c3e62d22\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-10.7.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.3/65.3 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting numba\n",
      "  Downloading numba-0.61.2-cp311-cp311-win_amd64.whl (2.8 MB)\n",
      "     ---------------------------------------- 2.8/2.8 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from openai-whisper==20240930) (1.24.3)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-win_amd64.whl (893 kB)\n",
      "     -------------------------------------- 893.9/893.9 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from openai-whisper==20240930) (2.0.0+cpu)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0\n",
      "  Downloading llvmlite-0.44.0-cp311-cp311-win_amd64.whl (30.3 MB)\n",
      "     ---------------------------------------- 30.3/30.3 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting regex>=2022.1.18\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from tiktoken->openai-whisper==20240930) (2.32.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (4.14.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch->openai-whisper==20240930) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from tqdm->openai-whisper==20240930) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from sympy->torch->openai-whisper==20240930) (1.3.0)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=813054 sha256=3fb08166fd2366db81fc14b0831a0a98ed37dda3803ea1ec707e7dddd7d59cdf\n",
      "  Stored in directory: C:\\Users\\ifedi\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-x3cyp8_9\\wheels\\1f\\1d\\98\\9583695e6695a6ac0ad42d87511097dce5ba486647dbfecb0e\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: tqdm, regex, more-itertools, llvmlite, tiktoken, numba, openai-whisper\n",
      "Successfully installed llvmlite-0.44.0 more-itertools-10.7.0 numba-0.61.2 openai-whisper-20240930 regex-2024.11.6 tiktoken-0.9.0 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\ifedi\\AppData\\Local\\Temp\\pip-req-build-y7g0svdv'\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "     -------------------------------------- 491.5/491.5 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.7.1-cp311-cp311-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "     ---------------------------------------- 10.5/10.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "     -------------------------------------- 362.1/362.1 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "     ---------------------------------------- 1.0/1.0 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting click>=8.1.8\n",
      "  Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Collecting rapidfuzz>=3.9.7\n",
      "  Downloading rapidfuzz-3.13.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "     ---------------------------------------- 25.8/25.8 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     -------------------------------------- 116.3/116.3 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 11.1/11.1 MB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "     -------------------------------------- 143.5/143.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "     -------------------------------------- 514.8/514.8 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from datasets) (25.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Collecting torch==2.7.1\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "     -------------------------------------- 216.1/216.1 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from torch==2.7.1->torchaudio) (3.1.6)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "     -------------------------------------- 181.4/181.4 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting pycparser\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "     -------------------------------------- 117.6/117.6 kB 3.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.12-cp311-cp311-win_amd64.whl (451 kB)\n",
      "     -------------------------------------- 451.4/451.4 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "     ---------------------------------------- 44.0/44.0 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.4-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "     ---------------------------------------- 41.5/41.5 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "     ---------------------------------------- 86.7/86.7 kB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\ifedi\\documents\\applied ai and ml\\session 2\\playground\\venv\\my_pytorch_cpu\\lib\\site-packages (from jinja2->torch==2.7.1->torchaudio) (3.0.2)\n",
      "Installing collected packages: pytz, xxhash, tzdata, safetensors, rapidfuzz, pyyaml, pycparser, pyarrow, propcache, multidict, fsspec, frozenlist, dill, click, aiohappyeyeballs, yarl, torch, pandas, multiprocess, jiwer, huggingface-hub, cffi, aiosignal, torchaudio, tokenizers, soundfile, aiohttp, accelerate, transformers, datasets\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.0+cpu\n",
      "    Uninstalling torch-2.0.0+cpu:\n",
      "      Successfully uninstalled torch-2.0.0+cpu\n",
      "Successfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.12 aiosignal-1.3.2 cffi-1.17.1 click-8.2.1 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 huggingface-hub-0.33.0 jiwer-3.1.0 multidict-6.4.4 multiprocess-0.70.16 pandas-2.3.0 propcache-0.3.2 pyarrow-20.0.0 pycparser-2.22 pytz-2025.2 pyyaml-6.0.2 rapidfuzz-3.13.0 safetensors-0.5.3 soundfile-0.13.1 tokenizers-0.21.1 torch-2.7.1 torchaudio-2.7.1 transformers-4.52.4 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.15.1+cpu requires torch==2.0.0, but you have torch 2.7.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Installing required packages\n",
    "\n",
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install jiwer datasets torchaudio transformers accelerate soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:08:21.803995Z",
     "iopub.status.busy": "2025-06-02T02:08:21.803729Z",
     "iopub.status.idle": "2025-06-02T02:08:21.808381Z",
     "shell.execute_reply": "2025-06-02T02:08:21.807585Z",
     "shell.execute_reply.started": "2025-06-02T02:08:21.803978Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ifedi\\Documents\\Applied AI and ML\\Session 2\\Playground\\venv\\my_pytorch_cpu\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# import dependent libraries\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import whisper\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from datasets import Dataset, DatasetDict\n",
    "from jiwer import wer, cer\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import torchaudio\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T01:40:12.529640Z",
     "iopub.status.busy": "2025-06-02T01:40:12.528920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/unitedsynmed/UnitedSynMed/transcript/validation.csv\n",
      "/kaggle/input/unitedsynmed/UnitedSynMed/transcript/train.csv\n",
      "/kaggle/input/unitedsynmed/UnitedSynMed/transcript/test.csv\n"
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "import os\n",
    "n = 0\n",
    "for dirname, _, filenames in os.walk('../data-collection/'):\n",
    "    for filename in filenames:\n",
    "        if n < 3:\n",
    "            print(os.path.join(dirname, filename))\n",
    "            n += 1\n",
    "        else: break\n",
    "    if n >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:08:26.954263Z",
     "iopub.status.busy": "2025-06-02T02:08:26.953453Z",
     "iopub.status.idle": "2025-06-02T02:08:31.600892Z",
     "shell.execute_reply": "2025-06-02T02:08:31.600323Z",
     "shell.execute_reply.started": "2025-06-02T02:08:26.954230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths to the dataset\n",
    "audio_root = \"/kaggle/input/unitedsynmed/UnitedSynMed/audio\"\n",
    "transcript_root = \"/kaggle/input/unitedsynmed/UnitedSynMed/transcript/\"\n",
    "\n",
    "# # Load CSVs and match them with audio paths\n",
    "# def load_split(split):\n",
    "#     csv_path = os.path.join(transcript_root, f\"{split}.csv\")\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     df[\"path\"] = df[\"file_name\"].apply(lambda x: os.path.join(audio_root, split, x))\n",
    "#     return df\n",
    "\n",
    "# # Create datasets\n",
    "# train_df = load_split(\"train\")\n",
    "# test_df = load_split(\"test\")\n",
    "# val_df = load_split(\"validation\")\n",
    "\n",
    "# # Convert to Hugging Face Dataset\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": Dataset.from_pandas(train_df),\n",
    "#     \"test\": Dataset.from_pandas(test_df),\n",
    "#     \"validation\": Dataset.from_pandas(val_df)\n",
    "# })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:08:58.456975Z",
     "iopub.status.busy": "2025-06-02T02:08:58.456312Z",
     "iopub.status.idle": "2025-06-02T02:08:58.464123Z",
     "shell.execute_reply": "2025-06-02T02:08:58.463337Z",
     "shell.execute_reply.started": "2025-06-02T02:08:58.456951Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': ['drug-female-defa7fcb-89d7-4b25-8834-90888b201d25.mp3',\n",
       "  'drug-female-160727b4-dd0c-43c7-ba17-963ae54347a0.mp3',\n",
       "  'drug-female-637d7dcc-fe73-499c-af76-b2ee28d36374.mp3',\n",
       "  'drug-male-02a2daf6-0f99-4939-848d-adc95f03d4bd.mp3',\n",
       "  'drug-brand-en-us-male-421229aa-4f71-48fa-bd43-a9ac606783f8.mp3'],\n",
       " 'transcription': ['Durysta is a medication used to reduce eye pressure in patients with open-angle glaucoma or ocular hypertension.',\n",
       "  'Annona muricata extract is known for its potential health benefits as a natural dietary supplement.',\n",
       "  'Many patients have found relief with REDBURY GOLD for their ongoing health issues.',\n",
       "  'ALMAL-Z is a popular medication used for treating allergies and cold symptoms.',\n",
       "  ' Norfazole may cause side effects such as nausea or a metallic taste in the mouth.'],\n",
       " 'path': ['/kaggle/input/unitedsynmed/UnitedSynMed/audio/train/drug-female-defa7fcb-89d7-4b25-8834-90888b201d25.mp3',\n",
       "  '/kaggle/input/unitedsynmed/UnitedSynMed/audio/train/drug-female-160727b4-dd0c-43c7-ba17-963ae54347a0.mp3',\n",
       "  '/kaggle/input/unitedsynmed/UnitedSynMed/audio/train/drug-female-637d7dcc-fe73-499c-af76-b2ee28d36374.mp3',\n",
       "  '/kaggle/input/unitedsynmed/UnitedSynMed/audio/train/drug-male-02a2daf6-0f99-4939-848d-adc95f03d4bd.mp3',\n",
       "  '/kaggle/input/unitedsynmed/UnitedSynMed/audio/train/drug-brand-en-us-male-421229aa-4f71-48fa-bd43-a9ac606783f8.mp3']}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T02:18:23.823683Z",
     "iopub.status.busy": "2025-06-02T02:18:23.823109Z",
     "iopub.status.idle": "2025-06-02T02:18:23.860457Z",
     "shell.execute_reply": "2025-06-02T02:18:23.859531Z",
     "shell.execute_reply.started": "2025-06-02T02:18:23.823661Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All audio resampled and saved to: C:/Users/ifedi/Documents/Applied AI and ML/Session 2/INFO8665 - Projects in Machine Learning/SageCare-2.0/data-collection/UnitedSynMed/audio_resampled\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "# Define source and target folders\n",
    "source_root = \"/kaggle/input/unitedsynmed/UnitedSynMed/audio\"\n",
    "target_root = \"/kaggle/input/unitedsynmed/UnitedSynMed/audio_resampled\"\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# splits = ['train', 'test', 'validation']\n",
    "\n",
    "# for split in splits:\n",
    "#     src_dir = os.path.join(source_root, split)\n",
    "#     tgt_dir = os.path.join(target_root, split)\n",
    "#     os.makedirs(tgt_dir, exist_ok=True)\n",
    "\n",
    "#     audio_files = glob.glob(os.path.join(src_dir, \"*.mp3\"))\n",
    "\n",
    "    audio_files = glob(os.path.join(src_dir, \"*.mp3\"))\n",
    "\n",
    "#         filename = os.path.splitext(os.path.basename(file))[0] + \".wav\"\n",
    "#         torchaudio.save(os.path.join(tgt_dir, filename), waveform, target_sample_rate)\n",
    "\n",
    "# print(\"✅ All audio resampled and saved to:\", target_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-02T02:28:57.003Z",
     "iopub.execute_input": "2025-06-02T02:19:38.165845Z",
     "iopub.status.busy": "2025-06-02T02:19:38.165539Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabe8a5820684812992243ef880239cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/632548 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load Whisper processor\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n",
    "\n",
    "# Set target sample rate\n",
    "target_sample_rate = 16000\n",
    "\n",
    "def preprocess(batch):\n",
    "    audio_input, sr = sf.read(batch[\"path\"])\n",
    "    \n",
    "    # If the sample rate is not 16kHz, resample it\n",
    "    waveform = torch.tensor(audio_input).float()\n",
    "    if len(waveform.shape) > 1 and waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0)  # Convert to mono\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sample_rate)\n",
    "    audio_input = resampler(waveform).numpy()\n",
    "    \n",
    "    inputs = processor(audio_input, sampling_rate=target_sample_rate, return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = inputs.input_features[0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"transcription\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id, -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-base\")\n",
    "# Freeze encoder layers\n",
    "for param in model.model.encoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper-medical\",\n",
    "    per_device_train_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer_score = wer(label_str, pred_str)\n",
    "    cer_score = cer(label_str, pred_str)\n",
    "\n",
    "    return {\"wer\": wer_score, \"cer\": cer_score}\n",
    "\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"whisper-medical-finetuned\")\n",
    "processor.save_pretrained(\"whisper-medical-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7569174,
     "sourceId": 12030144,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "my_pytorch_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
